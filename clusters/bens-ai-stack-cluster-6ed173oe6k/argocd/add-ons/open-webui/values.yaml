# Helm values for Open WebUI, applied by Ankra/ArgoCD.
# This file is the single source of truth for our overrides.

# 1. Enable embedded Ollama with qwen3 model
ollama:
  enabled: true
  fullnameOverride: "open-webui-ollama"
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1
  models:
    pull:
      - qwen3:latest
    run:
      - qwen3:latest
  persistentVolume:
    enabled: true
    size: 50Gi
  resources:
    requests:
      memory: 4Gi
      cpu: 1
    limits:
      memory: 8Gi
      cpu: 2
      nvidia.com/gpu: 1
  lifecycle:
    postStart:
      exec:
        command:
          - /bin/sh
          - -c
          - |
            sleep 15
            ollama pull qwen3:latest

# 2. Enable pipelines for OpenLIT integration
pipelines:
  enabled: true
  extraVolumes:
    - name: ollama-pipeline-config
      configMap:
        name: ollama-pipeline-config
  extraVolumeMounts:
    - name: ollama-pipeline-config
      mountPath: /app/pipelines/ollama.py
      subPath: ollama.py
      readOnly: true
  extraEnvVars:
    # This tells the pipeline script where the real Ollama service is
    - name: OLLAMA_BASE_URL
      value: "http://open-webui-ollama:11434"
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://otel-collector:4318"
    - name: OTEL_SERVICE_NAME
      value: "open-webui-pipelines"
    - name: OTEL_TRACES_EXPORTER
      value: "otlp_http"
    - name: OTEL_METRICS_EXPORTER
      value: "otlp_http"

# 3. Main Open WebUI configuration with RAG integration
extraEnvVars:
  # Default API key for pipelines
  - name: OPENAI_API_KEY
    value: "0p3n-w3bu!"
  
  # Enable general signup and disable authentication bypass
  - name: ENABLE_SIGNUP
    value: "true"
  - name: WEBUI_AUTH
    value: "true"
  - name: DEFAULT_USER_ROLE
    value: "pending"
  
  # OpenLIT/OTEL configuration
  - name: OTEL_EXPORTER_OTLP_ENDPOINT
    value: "http://otel-collector:4318"
  - name: OTEL_SERVICE_NAME
    value: "open-webui"
  - name: OTEL_TRACES_EXPORTER
    value: "otlp_http"
  - name: OTEL_METRICS_EXPORTER
    value: "otlp_http"

  # Database connection from secret
  - name: DATABASE_URL
    valueFrom:
      secretKeyRef:
        name: cnpg-rag-db-connection
        key: DATABASE_URL
  
  # RAG configuration with your existing KServe embedding infrastructure
  - name: RAG_VECTOR_DB
    value: "pgvector"
  - name: RAG_EMBEDDING_ENGINE
    value: ""
  - name: RAG_EMBEDDING_API_URL
    value: "http://embedding-minilm-l6-v2-transformer.kserve.svc.cluster.local/v2/models/embedding-minilm-l6-v2/infer"
  
  # Additional Open WebUI optimizations
  - name: JWT_SECRET
    value: "65b4f8472fd58d4bfe7b45ee8b3a951df8eac8a7ed8197f976d71eebda23b7bf"
  - name: ENABLE_RAG_WEB_SEARCH
    value: "true"
  - name: RAG_WEB_SEARCH_ENGINE
    value: "duckduckgo"

# 4. Standard image configuration
image:
  repository: ghcr.io/open-webui/open-webui
  tag: ""
  pullPolicy: "IfNotPresent"

# 5. Persistent volume for RAG file staging
persistence:
  enabled: true
  size: 50Gi

# --- Rest of configuration remains the same ---
nameOverride: ""
namespaceOverride: ""

tika:
  enabled: false

ollamaUrlsFromExtraEnv: true

websocket:
  enabled: false
  manager: redis
  url: redis://open-webui-redis:6379/0
  nodeSelector: {}
  redis:
    enabled: true
    name: open-webui-redis
    labels: {}
    annotations: {}
    pods:
      labels: {}
      annotations: {}
    image:
      repository: redis
      tag: 7.4.2-alpine3.21
      pullPolicy: IfNotPresent
    command: []
    args: []
    resources: {}
    service:
      containerPort: 6379
      type: ClusterIP
      labels: {}
      annotations: {}
      port: 6379
      nodePort: ""
    tolerations: []
    affinity: {}
    securityContext: {}

redis-cluster:
  enabled: false
  fullnameOverride: open-webui-redis
  auth:
    enabled: false
  replica:
    replicaCount: 3

clusterDomain: cluster.local
annotations: {}
podAnnotations: {}
podLabels: {}
replicaCount: 1
strategy: {}

serviceAccount:
  enable: true
  name: ""
  annotations: {}
  automountServiceAccountToken: false

imagePullSecrets: []
livenessProbe: {}
readinessProbe: {}
startupProbe: {}
resources: {}
copyAppData:
  resources: {}

managedCertificate:
  enabled: false
  name: "mydomain-chat-cert"
  domains:
    - chat.example.com

ingress:
  enabled: false
  class: ""
  annotations: {}
  host: "chat.example.com"
  additionalHosts: []
  tls: false
  existingSecret: ""
  extraLabels: {}

nodeSelector: {}
tolerations: []
affinity: {}
topologySpreadConstraints: []
hostAliases: []

service:
  type: ClusterIP
  annotations: {}
  port: 80
  containerPort: 8080
  nodePort: ""
  labels: {}
  loadBalancerClass: ""

enableOpenaiApi: true
openaiBaseApiUrl: "https://api.openai.com/v1"
openaiBaseApiUrls:
  - "http://open-webui-pipelines:9099"
commonEnvVars: []
runtimeClassName: ""
podSecurityContext: {}
containerSecurityContext: {}

sso:
  enabled: false
  enableSignup: true
  mergeAccountsByEmail: false
  enableRoleManagement: false
  enableGroupManagement: false
  google:
    enabled: false
    clientId: ""
    clientSecret: ""
    clientExistingSecret: ""
    clientExistingSecretKey: ""
  microsoft:
    enabled: false
    clientId: ""
    clientSecret: ""
    clientExistingSecret: ""
    clientExistingSecretKey: ""
    tenantId: ""
  github:
    enabled: false
    clientId: ""
    clientSecret: ""
    clientExistingSecret: ""
    clientExistingSecretKey: ""
  oidc:
    enabled: false
    clientId: ""
    clientSecret: ""
    clientExistingSecret: ""
    clientExistingSecretKey: ""
    providerUrl: ""
    providerName: "SSO"
    scopes: "openid email profile"
  roleManagement:
    rolesClaim: "roles"
    allowedRoles: ""
    adminRoles: ""
  groupManagement:
    groupsClaim: "groups"
  trustedHeader:
    enabled: false
    emailHeader: ""
    nameHeader: ""

extraResources: []
databaseUrl: ""

postgresql:
  enabled: false
  fullnameOverride: open-webui-postgres
  architecture: standalone
  auth:
    database: open-webui
    postgresPassword: 0p3n-w3bu!
    username: open-webui
    password: 0p3n-w3bu!
  primary:
    persistence:
      size: 1Gi
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 512Mi
        cpu: 500m

logging:
  level: ""
  components:
    audio: ""
    comfyui: ""
    config: ""
    db: ""
    images: ""
    main: ""
    models: ""
    ollama: ""
    openai: ""
    rag: ""
    webhook: ""

extraManifests:
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      name: ollama-pipeline-config
    data:
      ollama.py: |
        from typing import List, Union, Generator, Iterator
        from schemas import OpenAIChatMessage
        import requests
        import os

        class Pipeline:
            class Valves:
                # Add your custom parameters here.
                # These parameters will be sliders and inputs in the UI.
                pass

            def __init__(self):
                # This function is called when the server is started.
                # You can setup your models and other artifacts here.
                self.name = "Ollama Proxy"
                pass

            async def on_startup(self):
                # This function is called when the server is started.
                print(f"Starting up {self.name}...")
                pass

            async def on_shutdown(self):
                # This function is called when the server is stopped.
                print(f"Shutting down {self.name}...")
                pass

            def pipe(
                self, user_message: OpenAIChatMessage, model_id: str, messages: List[OpenAIChatMessage], body: dict
            ) -> Union[str, Generator, Iterator]:
                # This is where you can add your custom logic
                print(f"Proxying request for model: {model_id}")

                OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                
                try:
                    r = requests.post(
                        f"{OLLAMA_BASE_URL}/api/chat",
                        json={
                            "model": "qwen3:latest",
                            "messages": messages,
                            "stream": True,
                        },
                        stream=True,
                    )
                    r.raise_for_status()

                    return r.iter_content(chunk_size=8192)
                except Exception as e:
                    print(f"Error proxying request: {e}")
                    return f"Error: {e}"
